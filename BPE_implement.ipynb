{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BPE implementation"
      ],
      "metadata": {
        "id": "C7OcIRPwRsnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refenrences : https://github.com/DolbyUUU/byte_pair_encoding_BPE_subword_tokenization_implementation_python"
      ],
      "metadata": {
        "id": "TResAtfL7yqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xtjRMvfk73Gr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7mJYkYNQGgR"
      },
      "outputs": [],
      "source": [
        " # install and import libraries\n",
        "from collections import Counter, defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class BPE():\n",
        "    \"\"\"Byte-Pair Encoding: Subword-based tokenization algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, corpus, vocab_size):\n",
        "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
        "        self.corpus = corpus\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.word_freqs = defaultdict(int)\n",
        "        self.splits = {}\n",
        "        self.merges = {}\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train BPE tokenizer.\"\"\"\n",
        "\n",
        "        # compute the frequencies of each word in the corpus\n",
        "        for text in self.corpus:\n",
        "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "            new_words = [word for word, offset in words_with_offsets]\n",
        "            for word in new_words:\n",
        "                self.word_freqs[word] += 1\n",
        "\n",
        "        # compute the base vocabulary of all characters in the corpus\n",
        "        alphabet = []\n",
        "        for word in self.word_freqs.keys():\n",
        "            for letter in word:\n",
        "                if letter not in alphabet:\n",
        "                    alphabet.append(letter)\n",
        "        alphabet.sort()\n",
        "\n",
        "        # add the special token </w> at the beginning of the vocabulary\n",
        "        vocab = [\"</w>\"] + alphabet.copy()\n",
        "\n",
        "        # split each word into individual characters before training\n",
        "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
        "\n",
        "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
        "        while len(vocab) < self.vocab_size:\n",
        "\n",
        "            # compute the frequency of each pair\n",
        "            pair_freqs = self.compute_pair_freqs()\n",
        "\n",
        "            # find the most frequent pair\n",
        "            best_pair = \"\"\n",
        "            max_freq = None\n",
        "            for pair, freq in pair_freqs.items():\n",
        "                if max_freq is None or max_freq < freq:\n",
        "                    best_pair = pair\n",
        "                    max_freq = freq\n",
        "\n",
        "            # merge the most frequent pair\n",
        "            self.splits = self.merge_pair(*best_pair)\n",
        "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "            vocab.append(best_pair[0] + best_pair[1])\n",
        "        return self.merges\n",
        "\n",
        "\n",
        "    def compute_pair_freqs(self):\n",
        "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
        "\n",
        "        pair_freqs = defaultdict(int)\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            split = self.splits[word]\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "            for i in range(len(split) - 1):\n",
        "                pair = (split[i], split[i + 1])\n",
        "                pair_freqs[pair] += freq\n",
        "        return pair_freqs\n",
        "\n",
        "\n",
        "    def merge_pair(self, a, b):\n",
        "        \"\"\"Merge the given pair.\"\"\"\n",
        "\n",
        "        for word in self.word_freqs:\n",
        "            split = self.splits[word]\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == a and split[i + 1] == b:\n",
        "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            self.splits[word] = split\n",
        "        return self.splits\n",
        "\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize a given text with trained BPE tokenizer (including pre-tokenization, split, and merge).\"\"\"\n",
        "\n",
        "        pre_tokenize_result = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
        "\n",
        "        for pair, merge in self.merges.items():\n",
        "            for idx, split in enumerate(splits_text):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [merge] + split[i + 2 :]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits_text[idx] = split\n",
        "        result = sum(splits_text, [])\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and inference"
      ],
      "metadata": {
        "id": "1If5DPDuR3gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the Wikipedia corpus used for training\n",
        "with open('wiki_corpus.txt', encoding=\"utf8\") as f:\n",
        "    corpus = f.readlines()\n",
        "    print(corpus[:5])\n",
        "\n",
        "# set the hyperparameter of vocabulary size\n",
        "vocab_size = 1000\n",
        "\n",
        "# create a BPE tokenizer object\n",
        "MyBPE = BPE(corpus=corpus, vocab_size=vocab_size)\n",
        "\n",
        "# train BPE tokenizer with Wikipedia corpus\n",
        "MyBPE.train()\n",
        "\n",
        "# tokenize the given text\n",
        "# text = \"Love, hate, or feel meh about Harry Potter, it’s hard to argue that J.K. Rowling filled the books with intentional writing choices. From made up words to the meanings of names to the well-scripted first and last lines of each novel, Rowling wanted to the writing to match the intricate fantasy world she created for the now-iconic boy wizard. To examine a few of these choices, I’ll be taking a closer look at the first line of Harry Potter, as well as the last lines, from all of the Harry Potter novels.\"\n",
        "text = \"Byte-Pair Encoding (BPE) (subword-based tokenization) algorithm implementaions from scratch with python\"\n",
        "print(f\"\\nBPE tokenization result of text\\n'{text}'\")\n",
        "print(MyBPE.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N00ZhIf0QotB",
        "outputId": "0626db85-3168-4c8e-fe66-91e6cbce43f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['YMCA in South Australia\\n', \"South Australia (SA) \\xa0has a unique position in Australia's history as, unlike the other states which were founded as colonies, South Australia began as a self governing province Many were attracted to this and Adelaide and SA developed as an independent and free thinking state.\\n\", 'The compound of philosophical radicalism, evangelical religion and self reliant ability typical of its founders had given an equalitarian flavour to South Australian thinking from the beginning.\\n', 'It was into this social setting that in February 1850 a meeting was called primarily for the formation of an Association (apparently meaning a Y.M.C.A.)\\n', \"for apprentices and others, after their day's work, to enjoy books, lectures, discussions, readings, friendly relief and recreation for a leisure hour.\\n\"]\n",
            "\n",
            "BPE tokenization result of text\n",
            "'Byte-Pair Encoding (BPE) (subword-based tokenization) algorithm implementaions from scratch with python'\n",
            "['B', 'y', 'te', '-', 'P', 'air', 'En', 'c', 'od', 'ing', '(', 'B', 'P', 'E', ')', '(', 'sub', 'w', 'ord', '-', 'b', 'ased', 'to', 'k', 'en', 'iz', 'ation', ')', 'al', 'g', 'or', 'ith', 'm', 'im', 'p', 'le', 'ment', 'a', 'ions', 'from', 'sc', 'r', 'at', 'ch', 'with', 'p', 'y', 'th', 'on']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9Jtz-ncTpB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option 2\n"
      ],
      "metadata": {
        "id": "FQeC3__ETqD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refenrecens : https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/"
      ],
      "metadata": {
        "id": "EB9X8uUg7-uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Given a vocabulary (dictionary mapping words to frequency counts), returns a\n",
        "    dictionary of tuples representing the frequency count of pairs of characters\n",
        "    in the vocabulary.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"\n",
        "    Given a pair of characters and a vocabulary, returns a new vocabulary with the\n",
        "    pair of characters merged together wherever they appear.\n",
        "    \"\"\"\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "def get_vocab(data):\n",
        "    \"\"\"\n",
        "    Given a list of strings, returns a dictionary of words mapping to their frequency\n",
        "    count in the data.\n",
        "    \"\"\"\n",
        "    vocab = defaultdict(int)\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab\n",
        "\n",
        "def byte_pair_encoding(data, n):\n",
        "    \"\"\"\n",
        "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
        "    of characters found in the vocabulary of the input data.\n",
        "    \"\"\"\n",
        "    vocab = get_vocab(data)\n",
        "    for i in range(n):\n",
        "        pairs = get_stats(vocab)\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "dE0cL5w8Txdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "corpus = '''Tokenization is the process of breaking down\n",
        "a sequence of text into smaller units called tokens,\n",
        "which can be words, phrases, or even individual characters.\n",
        "Tokenization is often the first step in natural languages processing tasks\n",
        "such as text classification, named entity recognition, and sentiment analysis.\n",
        "The resulting tokens are typically used as input to further processing steps,\n",
        "such as vectorization, where the tokens are converted\n",
        "into numerical representations for machine learning models to use.'''\n",
        "data = corpus.split('.')\n",
        "\n",
        "n = 230\n",
        "bpe_pairs = byte_pair_encoding(data, n)\n",
        "bpe_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5uiTyLKT_52",
        "outputId": "efe8e7ba-6822-4978-b776-ac7d121156b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Tokenization</w>': 2,\n",
              " 'is</w>': 2,\n",
              " 'the</w>': 3,\n",
              " 'process</w>': 1,\n",
              " 'of</w>': 2,\n",
              " 'breaking</w>': 1,\n",
              " 'down</w>': 1,\n",
              " 'a</w>': 1,\n",
              " 'sequence</w>': 1,\n",
              " 'text</w>': 2,\n",
              " 'into</w>': 2,\n",
              " 'smaller</w>': 1,\n",
              " 'units</w>': 1,\n",
              " 'called</w>': 1,\n",
              " 'tokens,</w>': 1,\n",
              " 'which</w>': 1,\n",
              " 'can</w>': 1,\n",
              " 'be</w>': 1,\n",
              " 'words,</w>': 1,\n",
              " 'phrases,</w>': 1,\n",
              " 'or</w>': 1,\n",
              " 'even</w>': 1,\n",
              " 'individual</w>': 1,\n",
              " 'characters</w>': 1,\n",
              " 'often</w>': 1,\n",
              " 'first</w>': 1,\n",
              " 'step</w>': 1,\n",
              " 'in</w>': 1,\n",
              " 'natural</w>': 1,\n",
              " 'languages</w>': 1,\n",
              " 'processing</w>': 2,\n",
              " 'tasks</w>': 1,\n",
              " 'such</w>': 2,\n",
              " 'as</w>': 3,\n",
              " 'classification,</w>': 1,\n",
              " 'named</w>': 1,\n",
              " 'entity</w>': 1,\n",
              " 'recognition,</w>': 1,\n",
              " 'and</w>': 1,\n",
              " 'sentiment</w>': 1,\n",
              " 'analysis</w>': 1,\n",
              " 'The</w>': 1,\n",
              " 'resulting</w>': 1,\n",
              " 'tokens</w>': 2,\n",
              " 'are</w>': 2,\n",
              " 'typically</w>': 1,\n",
              " 'used</w>': 1,\n",
              " 'input</w>': 1,\n",
              " 'to</w>': 2,\n",
              " 'further</w>': 1,\n",
              " 'steps,</w>': 1,\n",
              " 'vectorization,</w>': 1,\n",
              " 'where</w>': 1,\n",
              " 'converted</w>': 1,\n",
              " 'numerical</w>': 1,\n",
              " 'representations</w>': 1,\n",
              " 'for</w>': 1,\n",
              " 'machine</w>': 1,\n",
              " 'learning</w>': 1,\n",
              " 'models</w>': 1,\n",
              " 'use</w>': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option 3"
      ],
      "metadata": {
        "id": "vrTjvmWN8H2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refenrecens : https://medium.com/@adari.girishkumar/unraveling-the-byte-pair-encoding-bpe-algorithm-in-nlp-39f82e48608c"
      ],
      "metadata": {
        "id": "hRZA_2JS8KW6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BujcFqgJZoSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cICu8ThOZoP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pI_AwpyDZoHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BPE implement\n"
      ],
      "metadata": {
        "id": "6Ue7GQuLZtPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_vocab(text):\n",
        "    # Khởi tạo từ vựng theo tần số của từng từ trong văn bản\n",
        "    vocab = Counter(text.split())\n",
        "    return {word: freq for word, freq in vocab.items()}\n",
        "\n",
        "def get_stats(vocab):\n",
        "    # Lấy tần số của các cặp ký hiệu liền kề (bigrams) trong từ vựng\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    # Hợp nhất cặp thường xuyên nhất trong tất cả các từ vựng và tần suất cập nhật\n",
        "    new_vocab = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "# Sample text data\n",
        "text = \"low lower newest widest\"\n",
        "# text = \"Byte-Pair Encoding (BPE) (subword-based tokenization) algorithm implementaions from scratch with python\"\n",
        "\n",
        "# Convert each word in initial vocabulary to space-separated string of characters\n",
        "vocab = get_vocab(text)\n",
        "vocab = {' '.join(word): freq for word, freq in vocab.items()}\n",
        "print(\"Initial vocabulary:\", vocab)\n",
        "\n",
        "# Number of BPE iterations\n",
        "num_merges = 100\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    # Get the most frequent pair\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best_pair, vocab)\n",
        "    print(f\"After iteration {i+1}, Best pair: {best_pair}\")\n",
        "    print(\"Updated vocabulary:\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX6ZKUzf8Jzf",
        "outputId": "4b9bfd06-68d9-4ce5-ae83-85cbad3058e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocabulary: {'l o w': 1, 'l o w e r': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
            "After iteration 1, Best pair: ('l', 'o')\n",
            "Updated vocabulary: {'lo w': 1, 'lo w e r': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
            "After iteration 2, Best pair: ('lo', 'w')\n",
            "Updated vocabulary: {'low': 1, 'low e r': 1, 'n e w e s t': 1, 'w i d e s t': 1}\n",
            "After iteration 3, Best pair: ('e', 's')\n",
            "Updated vocabulary: {'low': 1, 'low e r': 1, 'n e w es t': 1, 'w i d es t': 1}\n",
            "After iteration 4, Best pair: ('es', 't')\n",
            "Updated vocabulary: {'low': 1, 'low e r': 1, 'n e w est': 1, 'w i d est': 1}\n",
            "After iteration 5, Best pair: ('low', 'e')\n",
            "Updated vocabulary: {'low': 1, 'lowe r': 1, 'n e w est': 1, 'w i d est': 1}\n",
            "After iteration 6, Best pair: ('lowe', 'r')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'n e w est': 1, 'w i d est': 1}\n",
            "After iteration 7, Best pair: ('n', 'e')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'ne w est': 1, 'w i d est': 1}\n",
            "After iteration 8, Best pair: ('ne', 'w')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'new est': 1, 'w i d est': 1}\n",
            "After iteration 9, Best pair: ('new', 'est')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'newest': 1, 'w i d est': 1}\n",
            "After iteration 10, Best pair: ('w', 'i')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'newest': 1, 'wi d est': 1}\n",
            "After iteration 11, Best pair: ('wi', 'd')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'newest': 1, 'wid est': 1}\n",
            "After iteration 12, Best pair: ('wid', 'est')\n",
            "Updated vocabulary: {'low': 1, 'lower': 1, 'newest': 1, 'widest': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfhJn2Ga9dXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}